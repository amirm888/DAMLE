{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcbe593",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DAMLET C2 OOD EVALUATION CONFIG WITH OPTUNA BEST PARAMETERS\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. IMPORTS & SETUP\n",
    "# ==============================================================================\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from multiprocessing import freeze_support\n",
    "from typing import Union, Dict, List, Tuple\n",
    "from scipy.spatial.distance import cdist\n",
    "import shutil\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Use a non-interactive backend for saving figures\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, default_collate\n",
    "from transformers import ViTForImageClassification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "try:\n",
    "    from torch.cuda.amp import autocast\n",
    "except ImportError:\n",
    "    from torch.cpu.amp import autocast\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. CONFIGURATION - USING OPTUNA'S BEST PARAMETERS (TRIAL 21)\n",
    "# ==============================================================================\n",
    "class Config:\n",
    "    \"\"\"A centralized configuration class for the C_2 as OOD experiment using best parameters.\"\"\"\n",
    "    # --- Path Configuration ---\n",
    "    # Experts: C0, C1, C3, C4\n",
    "    MODEL_CHECKPOINTS: List[str] = [\n",
    "        r\"d:/new_patches/new_patches/C_0/model/checkpoint_epoch9_valloss0.0343.pth\",\n",
    "        r\"d:/new_patches/new_patches/C_1/model/checkpoint_epoch10_valloss0.0404.pth\", \n",
    "        r\"d:/new_patches/new_patches/C_3/model/checkpoint_epoch9_valloss0.0330_20250429_174446.pth\",\n",
    "        r\"D:/new_patches/new_patches/C_4/model/checkpoint_epoch7_valloss0.0344_20250429_133142.pth\" \n",
    "    ]\n",
    "    \n",
    "    # --- Experiment Configuration ---\n",
    "    TEST_SET_PATH: str = r\"D:\\new_patches\\new_patches\\C_2\"\n",
    "    \n",
    "    # --- OPTUNA BEST PARAMETERS (TRIAL 21 - Accuracy: 93.58%) ---\n",
    "    NUM_REPRESENTATIVE_SAMPLES: int = 29\n",
    "    NUM_CLOSEST_SAMPLES_PER_REP: int = 2300\n",
    "    \n",
    "    XGB_N_ESTIMATORS: int = 75\n",
    "    XGB_MAX_DEPTH: int = 2\n",
    "    XGB_LEARNING_RATE: float = 0.28819589755209746\n",
    "\n",
    "    MLP_HIDDEN_LAYERS: List[int] = [20, 4] # MLP_LAYER_1_SIZE: 20, MLP_LAYER_2_SIZE: 4\n",
    "    MLP_EPOCHS: int = 13\n",
    "    MLP_LR: float = 0.0012246613206706332\n",
    "\n",
    "    ATTENTION_EPOCHS: int = 23\n",
    "    ATTENTION_LR: float = 0.003003575085410412\n",
    "    # --- END OPTUNA PARAMETERS ---\n",
    "    \n",
    "    # --- Data Source Configuration ---\n",
    "    FEATURE_SPACE_EXCEL_PATH: str = r'D:\\.kaggle\\outputs_newRR5_vit_tsne_full09212025\\all_data_with_tsne.xlsx' \n",
    "    SOURCE_DATASET_NAMES: List[str] = [\"C_0\", \"C_1\", \"C_3\", \"C_4\"]\n",
    "    \n",
    "    # --- Visualization Configuration ---\n",
    "    # **ENABLED: Setting to True for final report generation.**\n",
    "    CREATE_VISUALIZATIONS: bool = True\n",
    "    VIZ_OTHER_SAMPLES_COUNT: int = 5000 \n",
    "    \n",
    "    # --- Output & Temporary Directories ---\n",
    "    # **NEW OUTPUT FOLDER for FINAL REPORT**\n",
    "    OUTPUT_DIR_BASE: str = r\"D:/.kaggle/C2_OOD_Evaluation_FINAL_REPORT\" \n",
    "    \n",
    "    # --- Model & Evaluation Parameters ---\n",
    "    BATCH_SIZE: int = 128 \n",
    "    NUM_WORKERS: int = 0\n",
    "    NUM_LABELS: int = 2\n",
    "    DEVICE: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    RANDOM_STATE: int = 42\n",
    "\n",
    "    # --- Expert Model Names for Logic ---\n",
    "    EXPERT_NAMES: List[str] = [\"C_0\", \"C_1\", \"C_3\", \"C_4\"]\n",
    "    TARGET_NAME: str = \"C_2\"\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. UTILITY FUNCTIONS & META-MODEL CLASSES\n",
    "# ==============================================================================\n",
    "def set_seed(seed: int):\n",
    "    \"\"\"Sets the random seed for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "class AttentionGatingNetwork(nn.Module):\n",
    "    def __init__(self, num_models: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(num_models * num_classes, num_models)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.softmax(self.layer(x))\n",
    "\n",
    "class MLPMetaModel(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_layers: List[int], output_size: int):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        current_size = input_size\n",
    "        for hidden_size in hidden_layers:\n",
    "            layers.append(nn.Linear(current_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            current_size = hidden_size\n",
    "        layers.append(nn.Linear(current_size, output_size))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "class ImageListDataset(Dataset):\n",
    "    \"\"\"A custom dataset to load a list of images and their labels from a DataFrame.\"\"\"\n",
    "    def __init__(self, dataframe: pd.DataFrame, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        image = robust_pil_loader(row['Path'])\n",
    "        label_str = str(row['Inner_Label'])\n",
    "        label_int = 1 if 'L_1' in label_str else 0\n",
    "        label = torch.tensor(label_int, dtype=torch.long)\n",
    "        \n",
    "        if image and self.transform:\n",
    "             image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "def print_section_header(title: str):\n",
    "    print(\"\\n\" + \"=\" * 80); print(f\"| {title.upper():^76} |\"); print(\"=\" * 80)\n",
    "\n",
    "def robust_pil_loader(path: str) -> Union[Image.Image, None]:\n",
    "    try:\n",
    "        with open(path, \"rb\") as f: return Image.open(f).convert(\"RGB\")\n",
    "    except (UnidentifiedImageError, OSError, FileNotFoundError): return None\n",
    "\n",
    "def get_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "def collate_fn_skip_corrupted(batch: list) -> Tuple:\n",
    "    \"\"\"\n",
    "    Custom collate function that filters out samples with corrupted images (None).\n",
    "    \"\"\"\n",
    "    valid_batch = [item for item in batch if item[0] is not None]\n",
    "    if not valid_batch:\n",
    "        return (None, None)\n",
    "    return default_collate(valid_batch)\n",
    "\n",
    "# **COORDINATE DETECTION (Crucial for stability)**\n",
    "def get_coordinate_columns(df: pd.DataFrame) -> Tuple[List[str], str, bool]:\n",
    "    \"\"\"Identifies the correct coordinate columns (tSNE or UMAP) regardless of case.\"\"\"\n",
    "    \n",
    "    available_cols = [col.upper() for col in df.columns]\n",
    "    \n",
    "    if 'TSNE_X' in available_cols and 'TSNE_Y' in available_cols:\n",
    "        x_col = df.columns[available_cols.index('TSNE_X')]\n",
    "        y_col = df.columns[available_cols.index('TSNE_Y')]\n",
    "        return [x_col, y_col], \"t-SNE\", True\n",
    "    elif 'UMAP_X' in available_cols and 'UMAP_Y' in available_cols:\n",
    "        x_col = df.columns[available_cols.index('UMAP_X')]\n",
    "        y_col = df.columns[available_cols.index('UMAP_Y')]\n",
    "        return [x_col, y_col], \"UMAP\", True\n",
    "    else:\n",
    "        # Returns default columns and False flag if necessary columns are missing.\n",
    "        print(\"❌ FATAL: Could not find UMAP or t-SNE columns in the Excel file.\")\n",
    "        return ['tSNE_X', 'tSNE_Y'], \"t-SNE (Missing)\", False\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. CORE COMPUTATIONAL, VISUALIZATION & REPORTING FUNCTIONS\n",
    "# ==============================================================================\n",
    "def find_representative_samples(test_set_df: pd.DataFrame, config: Config) -> Tuple[List[str], pd.DataFrame]:\n",
    "    \"\"\"Uses K-Means clustering on the provided test set to find archetypal samples.\"\"\"\n",
    "    \n",
    "    num_representatives = config.NUM_REPRESENTATIVE_SAMPLES\n",
    "    coord_cols, coord_name, found = get_coordinate_columns(test_set_df)\n",
    "\n",
    "    if not found:\n",
    "        return [], pd.DataFrame()\n",
    "        \n",
    "    print(f\"  > Clustering {len(test_set_df)} total samples into {num_representatives} groups based on {coord_name} coordinates alone.\")\n",
    "    \n",
    "    representative_paths = []\n",
    "    \n",
    "    if not test_set_df.empty and len(test_set_df) >= num_representatives:\n",
    "        coords = test_set_df[coord_cols].values\n",
    "        kmeans = KMeans(n_clusters=num_representatives, random_state=config.RANDOM_STATE, n_init=10).fit(coords)\n",
    "        \n",
    "        for centroid in kmeans.cluster_centers_:\n",
    "            distances = cdist(centroid.reshape(1, -1), coords, 'euclidean').flatten()\n",
    "            closest_idx = np.argmin(distances)\n",
    "            representative_paths.append(test_set_df.iloc[closest_idx]['Path'])\n",
    "    \n",
    "    representative_df = test_set_df[test_set_df['Path'].isin(representative_paths)]\n",
    "    return representative_paths, representative_df\n",
    "\n",
    "def find_closest_samples(target_sample_path: str, feature_space_df: pd.DataFrame, config: Config) -> pd.DataFrame:\n",
    "    \"\"\"Identifies the N closest source samples to a single target sample.\"\"\"\n",
    "    \n",
    "    num_closest = config.NUM_CLOSEST_SAMPLES_PER_REP\n",
    "    coord_cols, _, found = get_coordinate_columns(feature_space_df)\n",
    "    if not found:\n",
    "        return None\n",
    "        \n",
    "    target_filename = Path(target_sample_path).name\n",
    "    target_row = feature_space_df[feature_space_df['Filename'] == target_filename]\n",
    "    if target_row.empty: return None\n",
    "        \n",
    "    target_vector = target_row[coord_cols].values\n",
    "    source_df = feature_space_df[feature_space_df['Dataset'].isin(config.SOURCE_DATASET_NAMES)].copy()\n",
    "    if source_df.empty: return None\n",
    "\n",
    "    source_vectors = source_df[coord_cols].values\n",
    "    distances = cdist(target_vector, source_vectors, 'euclidean').flatten()\n",
    "    \n",
    "    source_df['Distance_to_Target'] = distances\n",
    "    closest_df = source_df.nsmallest(num_closest, 'Distance_to_Target')\n",
    "    return closest_df\n",
    "\n",
    "def load_models(config: Config) -> List[torch.nn.Module]:\n",
    "    print_section_header(\"Loading Base Models\")\n",
    "    models = []\n",
    "    for idx, path in enumerate(config.MODEL_CHECKPOINTS):\n",
    "        try:\n",
    "            model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224-in21k\", num_labels=config.NUM_LABELS, ignore_mismatched_sizes=True)\n",
    "            checkpoint = torch.load(path, map_location=config.DEVICE, weights_only=True)\n",
    "            model.load_state_dict(checkpoint.get('model_state_dict', checkpoint))\n",
    "            model.to(config.DEVICE).eval()\n",
    "            models.append(model)\n",
    "            print(f\"   > Model {config.EXPERT_NAMES[idx]} loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ FATAL: Could not load model from {path}: {e}. Exiting.\")\n",
    "            exit()\n",
    "    return models\n",
    "\n",
    "def get_predictions(models: List[torch.nn.Module], loader: DataLoader, config: Config, desc: str, leave_progress=True) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Gets predictions for a given dataloader.\"\"\"\n",
    "    num_models = len(models)\n",
    "    all_probs, true_labels = [[] for _ in range(num_models)], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=desc, leave=leave_progress):\n",
    "            if images is None: continue\n",
    "            images = images.to(config.DEVICE)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            # Fixed deprecated call from older code versions\n",
    "            with torch.amp.autocast('cuda', enabled=(config.DEVICE.type == 'cuda')): \n",
    "                for i, model in enumerate(models):\n",
    "                    logits = model(images).logits\n",
    "                    all_probs[i].extend(torch.softmax(logits, dim=1).cpu().numpy())\n",
    "    return np.array(all_probs), np.array(true_labels)\n",
    "\n",
    "def calculate_proximity_weights(dataset_counts: pd.Series, config: Config) -> np.ndarray:\n",
    "    \"\"\"Calculates model weights based on their dataset's proximity to the target.\"\"\"\n",
    "    total_samples = dataset_counts.sum()\n",
    "    weights = [dataset_counts.get(name, 0) / total_samples for name in config.EXPERT_NAMES]\n",
    "    return np.array(weights)\n",
    "\n",
    "def train_all_stacking_models(meta_features_np: np.ndarray, meta_train_labels: np.ndarray, config: Config, models: List[torch.nn.Module]) -> Dict[str, any]:\n",
    "    \n",
    "    trained_meta_models = {}\n",
    "    lr_model = LogisticRegression(random_state=config.RANDOM_STATE, max_iter=1000).fit(meta_features_np, meta_train_labels)\n",
    "    trained_meta_models[\"Stacking_LR\"] = lr_model\n",
    "\n",
    "    xgb_model = XGBClassifier(\n",
    "        n_estimators=config.XGB_N_ESTIMATORS, max_depth=config.XGB_MAX_DEPTH,\n",
    "        learning_rate=config.XGB_LEARNING_RATE, random_state=config.RANDOM_STATE,\n",
    "        eval_metric='logloss' \n",
    "    ).fit(meta_features_np, meta_train_labels)\n",
    "    trained_meta_models[\"Stacking_XGB\"] = xgb_model\n",
    "\n",
    "    mlp_model = MLPMetaModel(\n",
    "        input_size=len(models) * config.NUM_LABELS, hidden_layers=config.MLP_HIDDEN_LAYERS,\n",
    "        output_size=config.NUM_LABELS\n",
    "    ).to(config.DEVICE)\n",
    "    \n",
    "    mlp_dataset = TensorDataset(torch.from_numpy(meta_features_np).float(), torch.from_numpy(meta_train_labels).long())\n",
    "    mlp_loader = DataLoader(mlp_dataset, batch_size=config.BATCH_SIZE, shuffle=True)\n",
    "    optimizer = torch.optim.Adam(mlp_model.parameters(), lr=config.MLP_LR)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    mlp_model.train()\n",
    "    for epoch in tqdm(range(config.MLP_EPOCHS), desc=\"   => Training MLP\", leave=False):\n",
    "        for x_batch, y_batch in mlp_loader:\n",
    "            x_batch, y_batch = x_batch.to(config.DEVICE), y_batch.to(config.DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = mlp_model(x_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    mlp_model.eval()\n",
    "    trained_meta_models[\"Stacking_MLP\"] = mlp_model\n",
    "    return trained_meta_models\n",
    "\n",
    "def train_attention_network(train_probs: np.ndarray, train_labels: np.ndarray, config: Config, base_models: List[torch.nn.Module]) -> AttentionGatingNetwork:\n",
    "    \n",
    "    train_probs_t = torch.from_numpy(train_probs.swapaxes(0, 1)).to(config.DEVICE)\n",
    "    meta_features = train_probs_t.reshape(train_probs_t.shape[0], -1)\n",
    "    train_labels_t = torch.from_numpy(train_labels).long().to(config.DEVICE)\n",
    "    \n",
    "    attention_train_dataset = TensorDataset(meta_features.float(), train_probs_t.float(), train_labels_t)\n",
    "    attention_loader = DataLoader(attention_train_dataset, batch_size=config.BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    attention_net = AttentionGatingNetwork(len(base_models), config.NUM_LABELS).to(config.DEVICE)\n",
    "    optimizer = torch.optim.Adam(attention_net.parameters(), lr=config.ATTENTION_LR)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    attention_net.train()\n",
    "    for epoch in tqdm(range(config.ATTENTION_EPOCHS), desc=\"   => Training Attention Net\", leave=False):\n",
    "        for x_batch, probs_batch, y_batch in attention_loader:\n",
    "            optimizer.zero_grad()\n",
    "            attention_weights = attention_net(x_batch)\n",
    "            weighted_probs = probs_batch * attention_weights.unsqueeze(-1)\n",
    "            final_probs = torch.sum(weighted_probs, dim=1)\n",
    "            loss = criterion(final_probs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "    attention_net.eval()\n",
    "    return attention_net\n",
    "\n",
    "def apply_ensembles(all_probs: np.ndarray, proximity_weights: np.ndarray, attention_net: AttentionGatingNetwork, config: Config) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"Applies all specialized and standard ensemble methods.\"\"\"\n",
    "    all_preds = np.argmax(all_probs, axis=2)\n",
    "    ensembles = {\"Majority Vote\": np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=all_preds)}\n",
    "\n",
    "    if proximity_weights is not None and proximity_weights.any():\n",
    "        ensembles[\"Proximity_Weighted (Soft)\"] = np.argmax(np.einsum('j,ijk->ik', proximity_weights, all_probs.transpose(1, 0, 2)), axis=1)\n",
    "\n",
    "    if attention_net:\n",
    "        test_probs_t = torch.from_numpy(all_probs.swapaxes(0, 1)).to(config.DEVICE)\n",
    "        meta_features_test = test_probs_t.reshape(test_probs_t.shape[0], -1)\n",
    "        with torch.no_grad():\n",
    "            attention_weights = attention_net(meta_features_test.float())\n",
    "        weighted_probs = test_probs_t.cpu().numpy() * attention_weights.cpu().numpy()[:, :, np.newaxis]\n",
    "        final_probs = np.sum(weighted_probs, axis=1)\n",
    "        ensembles[\"Attention_Ensemble (Specialized)\"] = np.argmax(final_probs, axis=1)\n",
    "\n",
    "    return ensembles\n",
    "\n",
    "def create_detailed_visualizations(feature_space_df: pd.DataFrame, specific_test_df: pd.DataFrame, representative_paths: List[str], representative_df: pd.DataFrame, config: Config):\n",
    "    \"\"\"Generates a series of plots to visualize the feature space.\"\"\"\n",
    "    print_section_header(\"Generating Visualizations\")\n",
    "    viz_dir = Path(config.OUTPUT_DIR_BASE) / \"visualizations\"\n",
    "    viz_dir.mkdir(exist_ok=True, parents=True)\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    coord_cols, coord_name, _ = get_coordinate_columns(feature_space_df)\n",
    "    coord_x, coord_y = coord_cols[0], coord_cols[1]\n",
    "\n",
    "    # --- Plot 1: Overall Feature Space of All Datasets ---\n",
    "    plt.figure(figsize=(18, 14))\n",
    "    sns.scatterplot(x=coord_x, y=coord_y, hue='Dataset', data=feature_space_df, palette='tab10', alpha=0.6, s=15, legend='full')\n",
    "    plt.title(f'Global {coord_name} Feature Space of All Datasets', fontsize=20, weight='bold')\n",
    "    plt.xlabel(f'{coord_name} Dimension 1', fontsize=12); plt.ylabel(f'{coord_name} Dimension 2', fontsize=12)\n",
    "    plt.legend(title='Dataset', loc='best', markerscale=2)\n",
    "    plt.savefig(viz_dir / \"01_global_feature_space.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"  > Saved plot 1: Global Feature Space\")\n",
    "\n",
    "    # --- Plot 3: OOD Test Set with Class Labels and Representative Anchors ---\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    sns.scatterplot(x=coord_x, y=coord_y, hue='Inner_Label', data=specific_test_df, palette={'L_0': 'royalblue', 'L_1': 'crimson'}, alpha=0.5, s=20)\n",
    "    \n",
    "    if not representative_df.empty:\n",
    "        plt.scatter(representative_df[coord_x], representative_df[coord_y], marker='*', s=400, c='gold', edgecolor='black', linewidth=1, label='Representative Anchors')\n",
    "    \n",
    "        path_to_r_number = {path: f'R{i+1}' for i, path in enumerate(representative_paths)}\n",
    "        for i, row in representative_df.iterrows():\n",
    "            try:\n",
    "                anchor_index = representative_paths.index(row['Path']) + 1\n",
    "                plt.text(row[coord_x] + 0.5, row[coord_y] + 0.5, f'R{anchor_index}', fontsize=12, weight='bold', color='black')\n",
    "            except (ValueError, IndexError):\n",
    "                continue\n",
    "\n",
    "    plt.title(f'Distribution of {config.TARGET_NAME} with {len(representative_paths)} Anchors', fontsize=18, weight='bold')\n",
    "    plt.xlabel(f'{coord_name} Dimension 1'); plt.ylabel(f'{coord_name} Dimension 2')\n",
    "    plt.legend(title='Class')\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.savefig(viz_dir / \"03_test_set_with_anchors_kmeans.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"  > Saved plot 3: OOD Test Set with Anchors (KMeans)\")\n",
    "\n",
    "    # --- Plots 4-N: Individual Anchor Neighborhoods ---\n",
    "    source_df = feature_space_df[feature_space_df['Dataset'].isin(config.SOURCE_DATASET_NAMES)].copy()\n",
    "    path_to_r_number = {path: f'R{i+1}' for i, path in enumerate(representative_paths)}\n",
    "    \n",
    "    for path in tqdm(representative_paths, desc=\"  > Generating Anchor Neighborhood Plots\"):\n",
    "        closest_df = find_closest_samples(path, feature_space_df, config)\n",
    "        if closest_df is None: continue\n",
    "        \n",
    "        anchor_row = feature_space_df[feature_space_df['Path'] == path]\n",
    "        anchor_name = path_to_r_number.get(path, 'Unknown')\n",
    "        \n",
    "        plt.figure(figsize=(16, 12))\n",
    "        other_source_df = source_df.drop(closest_df.index).sample(n=min(config.VIZ_OTHER_SAMPLES_COUNT, len(source_df) - len(closest_df)), random_state=config.RANDOM_STATE)\n",
    "        sns.scatterplot(x=coord_x, y=coord_y, data=other_source_df, color='gainsboro', alpha=0.5, label='Other Source Samples', s=15)\n",
    "        \n",
    "        sns.scatterplot(x=coord_x, y=coord_y, hue='Dataset', data=closest_df, palette='viridis', alpha=0.9, s=50, legend='full')\n",
    "\n",
    "        plt.scatter(anchor_row[coord_x], anchor_row[coord_y], marker='*', s=700, c='red', edgecolor='black', linewidth=1.5, label=f'Anchor {anchor_name}')\n",
    "        \n",
    "        plt.title(f'KNN Neighborhood for Anchor {anchor_name}: {Path(path).name} (N={config.NUM_CLOSEST_SAMPLES_PER_REP})', fontsize=16, weight='bold')\n",
    "        plt.xlabel(f'{coord_name} Dimension 1'); plt.ylabel(f'{coord_name} Dimension 2')\n",
    "        plt.legend(title='Dataset')\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        plt.savefig(viz_dir / f\"04_{anchor_name}_anchor_{Path(path).stem}.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def compute_full_metrics(true_labels: np.ndarray, preds: np.ndarray) -> Dict[str, any]:\n",
    "    \"\"\"Computes a detailed dictionary of classification metrics.\"\"\"\n",
    "    cm = confusion_matrix(true_labels, preds, labels=[0, 1])\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0,0,0,0)\n",
    "    \n",
    "    precision_0 = tn / (tn + fn) if (tn + fn) > 0 else 0.0\n",
    "    precision_1 = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall_0 = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    recall_1 = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1_0 = 2 * (precision_0 * recall_0) / (precision_0 + recall_0) if (precision_0 + recall_0) > 0 else 0.0\n",
    "    f1_1 = 2 * (precision_1 * recall_1) / (precision_1 + recall_1) if (precision_1 + recall_1) > 0 else 0.0\n",
    "    \n",
    "    class0_mask, class1_mask = true_labels == 0, true_labels == 1\n",
    "    \n",
    "    return {\n",
    "        \"acc\": 100 * np.sum(preds == true_labels) / len(true_labels) if len(true_labels) > 0 else 0.0,\n",
    "        \"class0_acc\": 100 * np.sum(preds[class0_mask] == true_labels[class0_mask]) / class0_mask.sum() if class0_mask.sum() > 0 else 0.0,\n",
    "        \"class1_acc\": 100 * np.sum(preds[class1_mask] == true_labels[class1_mask]) / class1_mask.sum() if class1_mask.sum() > 0 else 0.0,\n",
    "        \"precision\": [precision_0, precision_1], \"recall\": [recall_0, recall_1], \n",
    "        \"f1\": [f1_0, f1_1], \"correct\": int(np.sum(preds == true_labels)), \"total\": len(true_labels)\n",
    "    }\n",
    "\n",
    "def generate_final_report(all_predictions: Dict[str, np.ndarray], true_labels: np.ndarray, class_names: List[str], config: Config):\n",
    "    \"\"\"Generates a detailed summary report and saves it as an image.\"\"\"\n",
    "    print_section_header(\"Master Summary Report\")\n",
    "    \n",
    "    summary_data = []\n",
    "    method_order = sorted(all_predictions.keys())\n",
    "\n",
    "    for name in method_order:\n",
    "        metrics = compute_full_metrics(true_labels, all_predictions[name])\n",
    "        summary_data.append({\n",
    "            \"Method\": name, \n",
    "            \"Acc (%)\": metrics['acc'], # Store as number for sorting\n",
    "            f\"{class_names[0]} Acc (%)\": f\"{metrics['class0_acc']:.2f}\", \n",
    "            f\"{class_names[1]} Acc (%)\": f\"{metrics['class1_acc']:.2f}\",\n",
    "            \"Prec C0\": f\"{metrics['precision'][0]:.2f}\", \"Prec C1\": f\"{metrics['precision'][1]:.2f}\",\n",
    "            \"Rec C0\": f\"{metrics['recall'][0]:.2f}\", \"Rec C1\": f\"{metrics['recall'][1]:.2f}\",\n",
    "            \"F1 C0\": f\"{metrics['f1'][0]:.2f}\", \"F1 C1\": f\"{metrics['f1'][1]:.2f}\",\n",
    "            \"Correct/Total\": f\"{metrics['correct']}/{metrics['total']}\"\n",
    "        })\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    # Sort the DataFrame by Accuracy in descending order\n",
    "    summary_df = summary_df.sort_values(by='Acc (%)', ascending=False).reset_index(drop=True)\n",
    "    # Format the accuracy column back to a string with 2 decimal places for display\n",
    "    summary_df['Acc (%)'] = summary_df['Acc (%)'].map('{:.2f}'.format)\n",
    "\n",
    "    print(summary_df.to_string(index=False))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20, (len(summary_df) * 0.4 + 1.5)))\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "    table = ax.table(cellText=summary_df.values, colLabels=summary_df.columns, loc='center', cellLoc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(8)\n",
    "    table.scale(1, 1.8)\n",
    "    plt.title(f\"Evaluation Summary for {config.TARGET_NAME} (Optimized - Acc: {summary_df.iloc[0]['Acc (%)']}%)\", fontsize=16, y=1.05)\n",
    "    summary_image_path = Path(config.OUTPUT_DIR_BASE) / \"final_evaluation_summary.png\"\n",
    "    plt.savefig(summary_image_path, bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"\\n✅ Detailed summary table saved as an image to: {summary_image_path}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. MAIN EXECUTION SCRIPT\n",
    "# ==============================================================================\n",
    "def main():\n",
    "    \"\"\"Main function to orchestrate the entire evaluation pipeline.\"\"\"\n",
    "    config = Config()\n",
    "    Path(config.OUTPUT_DIR_BASE).mkdir(exist_ok=True, parents=True)\n",
    "    set_seed(config.RANDOM_STATE) \n",
    "    \n",
    "    print_section_header(\"Pipeline Start: Final Report Generation (Optimized Parameters)\")\n",
    "    \n",
    "    # --- 1. Pre-load Static Data (Models and Feature Space) ---\n",
    "    base_models = load_models(config)\n",
    "    try:\n",
    "        feature_space_df = pd.read_excel(config.FEATURE_SPACE_EXCEL_PATH)\n",
    "        feature_space_df['Path'] = feature_space_df['Path'].astype(str)\n",
    "        print(f\"✅ Pre-loaded coordinate data from {config.FEATURE_SPACE_EXCEL_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ FATAL ERROR loading coordinate Excel file: {e}\"); exit()\n",
    "        \n",
    "    # --- 2. Define the Specific Test Set ---\n",
    "    test_set_filenames = {p.name for p in Path(config.TEST_SET_PATH).rglob('*') if p.suffix in ['.png', '.jpg', '.jpeg', '.tif']}\n",
    "    specific_test_df = feature_space_df[feature_space_df['Filename'].isin(test_set_filenames)].copy()\n",
    "\n",
    "    if specific_test_df.empty:\n",
    "        print(f\"❌ FATAL: No matching filenames found between folder '{config.TEST_SET_PATH}' and the Excel file.\")\n",
    "        exit()\n",
    "    \n",
    "    # --- 3. Run Predictions on Test Set ONCE (Heavy Step) ---\n",
    "    print_section_header(f\"Running Predictions for Test Set ({len(specific_test_df)} images)...\")\n",
    "    full_test_dataset = ImageListDataset(dataframe=specific_test_df, transform=get_transform())\n",
    "    full_test_loader = DataLoader(full_test_dataset, batch_size=config.BATCH_SIZE, num_workers=config.NUM_WORKERS, collate_fn=collate_fn_skip_corrupted)\n",
    "    \n",
    "    all_test_probs, true_labels = get_predictions(base_models, full_test_loader, config, \"   => Final Test Set Preds\", leave_progress=True)\n",
    "    print(\"✅ Prediction complete.\")\n",
    "    \n",
    "    # --- 4. Build Aggregate Neighbor Dataset for Training Meta-Models ---\n",
    "    print_section_header(\"Building Aggregate Neighbor Dataset\")\n",
    "    representative_paths, representative_df = find_representative_samples(specific_test_df, config)\n",
    "\n",
    "    all_neighbor_dfs = []\n",
    "    for path in representative_paths:\n",
    "        closest_df = find_closest_samples(path, feature_space_df, config)\n",
    "        if closest_df is not None:\n",
    "            all_neighbor_dfs.append(closest_df)\n",
    "    \n",
    "    if not all_neighbor_dfs:\n",
    "        print(\"❌ FATAL: Could not find any neighbors for the representative samples. Halting.\"); exit()\n",
    "\n",
    "    aggregate_neighbors_df = pd.concat(all_neighbor_dfs).drop_duplicates(subset=['Filename']).reset_index(drop=True)\n",
    "    dataset_counts = aggregate_neighbors_df['Dataset'].value_counts()\n",
    "    print(f\"  > Created aggregate training set with {len(aggregate_neighbors_df)} unique neighbors.\")\n",
    "    \n",
    "    # --- 5. Train specialized meta-models ---\n",
    "    print_section_header(\"Training Meta-Models with Optimized Parameters\")\n",
    "    transform = get_transform()\n",
    "    neighbor_dataset = ImageListDataset(dataframe=aggregate_neighbors_df, transform=transform)\n",
    "    neighbor_loader = DataLoader(neighbor_dataset, batch_size=config.BATCH_SIZE, num_workers=config.NUM_WORKERS, collate_fn=collate_fn_skip_corrupted)\n",
    "    \n",
    "    meta_train_probs, meta_train_labels = get_predictions(base_models, neighbor_loader, config, \"   => Neighbor Preds\", leave_progress=True)\n",
    "    \n",
    "    meta_features_np = meta_train_probs.swapaxes(0, 1).reshape(meta_train_probs.shape[1], -1)\n",
    "    \n",
    "    trained_meta_models = train_all_stacking_models(meta_features_np, meta_train_labels, config, base_models)\n",
    "    attention_network = train_attention_network(meta_train_probs, meta_train_labels, config, base_models)\n",
    "    proximity_weights = calculate_proximity_weights(dataset_counts, config)\n",
    "\n",
    "    # --- 6. Apply all ensemble methods on the Test Set ---\n",
    "    print_section_header(\"Applying Ensemble Models\")\n",
    "    all_predictions = {f\"Model_{name}\": np.argmax(all_test_probs[i], axis=1) for i, name in enumerate(config.EXPERT_NAMES)}\n",
    "    all_predictions.update(apply_ensembles(all_test_probs, proximity_weights, attention_network, config))\n",
    "\n",
    "    # --- Apply the specialized stacking models ---\n",
    "    if trained_meta_models:\n",
    "        meta_features_test = all_test_probs.swapaxes(0, 1).reshape(all_test_probs.shape[1], -1)\n",
    "        for name, meta_model in trained_meta_models.items():\n",
    "            if isinstance(meta_model, (LogisticRegression, XGBClassifier)):\n",
    "                all_predictions[name] = meta_model.predict(meta_features_test)\n",
    "            elif isinstance(meta_model, nn.Module):\n",
    "                with torch.no_grad():\n",
    "                    meta_features_t = torch.from_numpy(meta_features_test).float().to(config.DEVICE)\n",
    "                    outputs = meta_model(meta_features_t)\n",
    "                    all_predictions[name] = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "\n",
    "    # --- 7. Final Reporting & Saving Results ---\n",
    "    generate_final_report(all_predictions, true_labels, ['L_0', 'L_1'], config)\n",
    "\n",
    "    if config.CREATE_VISUALIZATIONS:\n",
    "        print_section_header(\"Generating Feature Space Visualizations\")\n",
    "        create_detailed_visualizations(feature_space_df, specific_test_df, representative_paths, representative_df, config)\n",
    "\n",
    "    # Save per-image results to Excel\n",
    "    processed_paths_df = specific_test_df.iloc[:len(true_labels)]\n",
    "    summary_df = pd.DataFrame({'ImagePath': processed_paths_df['Path'].tolist(), 'TrueLabel': true_labels})\n",
    "    for method, preds in all_predictions.items():\n",
    "        summary_df[f'{method}_Pred'] = preds\n",
    "    \n",
    "    summary_path = Path(config.OUTPUT_DIR_BASE) / f\"per_image_results_FINAL_REPORT.xlsx\"\n",
    "    summary_df.to_excel(summary_path, index=False)\n",
    "    print(f\"\\n✅ Per-image prediction results saved to: {summary_path}\")\n",
    "\n",
    "    print_section_header(\"PIPELINE FINISHED - REPORT GENERATED\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    freeze_support()\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
