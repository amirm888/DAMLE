{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0a8155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SCRIPT: Extract_ViT_Features_With_Path_and_Labels.py\n",
    "# DESCRIPTION: This script extracts Vision Transformer (ViT) features from images \n",
    "#              in a dataset organized by hospital classes (C_0 to C_4) with subfolders\n",
    "#              (L_0 or L_1). It saves feature vectors along with full file paths, \n",
    "#              dataset labels (C_0 to C_4), and inner labels (L_0 or L_1) in .pkl files\n",
    "#              for each hospital. The script uses a pretrained ViT model from Hugging Face.\n",
    "# AUTHOR: Amir Soleimani-Yazdi\n",
    "# DATE: 2025-09-19\n",
    "# ==============================================================================\n",
    "\n",
    "# --- 1. Imports ---\n",
    "# ------------------------------------------------------------------------------\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoImageProcessor, ViTModel\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "# Other imports for potential downstream analysis (kept for compatibility)\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "# --- 2. Load Pretrained ViT Model and Image Processor ---\n",
    "# ------------------------------------------------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the Hugging Face model name\n",
    "model_name = \"google/vit-base-patch16-224-in21k\"\n",
    "\n",
    "# Load the ViT model and its corresponding image processor\n",
    "model = ViTModel.from_pretrained(model_name).to(device)\n",
    "image_processor = AutoImageProcessor.from_pretrained(model_name, use_fast=True)\n",
    "model.eval()\n",
    "print(f\"âœ… ViT model '{model_name}' loaded on {device}.\")\n",
    "\n",
    "# --- 3. Feature Extraction Function for ViT ---\n",
    "# ------------------------------------------------------------------------------\n",
    "def extract_features_vit(image_paths, model, processor, hospital_id):\n",
    "    features = []\n",
    "    for path in tqdm(image_paths, desc=f\"Extracting ViT features for {hospital_id}\"):\n",
    "        try:\n",
    "            img = Image.open(path).convert('RGB')\n",
    "            \n",
    "            # Use the ViT image processor\n",
    "            inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Get the model's output\n",
    "                outputs = model(**inputs)\n",
    "                \n",
    "                # Use the .pooler_output for the [CLS] token representation (shape: 768,)\n",
    "                feature = outputs.pooler_output.squeeze().cpu().numpy()\n",
    "                \n",
    "                # Extract inner label (L_0 or L_1) from the path\n",
    "                inner_label = 'L_0' if 'L_0' in path else 'L_1' if 'L_1' in path else 'Unknown'\n",
    "                \n",
    "                # Store tuple: (full_path, feature_vector, dataset_label, inner_label)\n",
    "                features.append((path, feature, hospital_id, inner_label))\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error with {path}: {e}\")\n",
    "    return features\n",
    "\n",
    "# --- 4. Extract and Save Features for Each Hospital ---\n",
    "# ------------------------------------------------------------------------------\n",
    "dataset_dir = r\"d:\\new_patches\\new_patches\"\n",
    "hospital_features = {}\n",
    "\n",
    "# Define output directory for .pkl files (same as t-SNE script for compatibility)\n",
    "output_dir = r\"d:\\.kaggle\\new_FE_ViT_with_Path_and_Labels\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for i in range(5):\n",
    "    hospital_id = f\"C_{i}\"\n",
    "    hospital_path = os.path.join(dataset_dir, hospital_id)\n",
    "\n",
    "    image_paths = []\n",
    "    for root, _, files in os.walk(hospital_path):\n",
    "        image_paths += [os.path.join(root, f) for f in files if f.lower().endswith(('png', 'jpg', 'jpeg'))]\n",
    "\n",
    "    print(f\"\\nâž¡ï¸ {hospital_id} - Total Images: {len(image_paths)}\")\n",
    "\n",
    "    if image_paths:\n",
    "        # Call the feature extraction function with hospital_id\n",
    "        feature_vectors = extract_features_vit(image_paths, model, image_processor, hospital_id)\n",
    "        \n",
    "        # Save to .pkl file in the output directory\n",
    "        output_filename = os.path.join(output_dir, f\"features_vit_{hospital_id}.pkl\")\n",
    "        with open(output_filename, \"wb\") as f:\n",
    "            pickle.dump(feature_vectors, f)\n",
    "            \n",
    "        hospital_features[hospital_id] = feature_vectors\n",
    "        print(f\"ðŸ’¾ Saved {output_filename}\")\n",
    "\n",
    "print(\"\\n--- Feature Extraction Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9919e3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#looading and preparing latent vectors and Tsne coordinations \n",
    "\n",
    "# ==============================================================================\n",
    "# SCRIPT: ViT_Feature_tSNE_Analysis_Full_C2_Distances.py\n",
    "# DESCRIPTION: This script performs t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "#              on the full set of Vision Transformer (ViT) feature vectors from all classes\n",
    "#              (C_0 to C_4). It generates an Excel file with filename, path, t-SNE coordinates,\n",
    "#              dataset (C_0 to C_4), inner label (L_0 or L_1), and distances to all centroids\n",
    "#              for all samples. A separate Excel file is generated for C_2 samples' distances\n",
    "#              to centroids. Visualizations (scatter plot, heatmaps) use subsampled points for\n",
    "#              efficiency. Outputs are saved in a new folder for full t-SNE results.\n",
    "# AUTHOR:\n",
    "# DATE: 2025-09-19\n",
    "# ==============================================================================\n",
    "\n",
    "# --- 1. Import Necessary Libraries ---\n",
    "# ------------------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import pdist, squareform, cosine, cdist\n",
    "\n",
    "# --- 2. Configuration and Setup ---\n",
    "# ------------------------------------------------------------------------------\n",
    "# Define the base directory where feature files are located.\n",
    "FEATURE_DIR = r\"d:\\.kaggle\"\n",
    "\n",
    "# Define the directory where all outputs (plots, data files) will be saved.\n",
    "OUTPUT_DIR = r\"d:\\.kaggle\\outputs_newRR5_vit_tsne_full09192025\"\n",
    "\n",
    "# Create the output directory if it doesn't already exist.\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Subsampling configuration (for visualization only)\n",
    "SUBSAMPLE_SIZE = 1000  # Maximum total samples for visualization\n",
    "SUBSAMPLE_FRACTION = 0.1  # Fraction per class for visualization\n",
    "\n",
    "print(\"--- Configuration Complete ---\")\n",
    "print(f\"Feature source directory: {FEATURE_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\\n\")\n",
    "\n",
    "# --- 3. Data Loading and Preprocessing ---\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"--- Step 1: Loading and Preparing Data ---\")\n",
    "\n",
    "# A dictionary to hold the feature vectors and filenames for each class/hospital.\n",
    "all_features = {}\n",
    "all_filenames = {}\n",
    "\n",
    "# Initialize empty lists to aggregate data from all files.\n",
    "labels = []\n",
    "feature_vectors = []\n",
    "filenames = []\n",
    "\n",
    "# Loop through the 5 classes (C_0 to C_4).\n",
    "for i in range(5):\n",
    "    hospital_id = f\"C_{i}\"\n",
    "    \n",
    "    # Construct the full path to the feature file.\n",
    "    feature_file_path = os.path.join(FEATURE_DIR, f\"features_vit_{hospital_id}.pkl\")\n",
    "\n",
    "    # Check if the file exists before trying to open it.\n",
    "    if os.path.exists(feature_file_path):\n",
    "        print(f\"--> Loading features for {hospital_id} from {feature_file_path}...\")\n",
    "        try:\n",
    "            with open(feature_file_path, \"rb\") as f:\n",
    "                # Load the data from the pickle file.\n",
    "                data = pickle.load(f)\n",
    "                \n",
    "                # Extract the feature vectors and filenames.\n",
    "                class_features = [item[1] for item in data]\n",
    "                class_filenames = [item[0] for item in data]\n",
    "                \n",
    "                # Store the loaded features and filenames.\n",
    "                all_features[hospital_id] = class_features\n",
    "                all_filenames[hospital_id] = class_filenames\n",
    "                \n",
    "                # Append features, filenames, and corresponding labels to master lists.\n",
    "                feature_vectors.extend(class_features)\n",
    "                filenames.extend(class_filenames)\n",
    "                labels.extend([hospital_id] * len(class_features))\n",
    "                \n",
    "                print(f\"    âœ… Loaded {len(class_features)} feature vectors for {hospital_id}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"    âš ï¸ Error loading {feature_file_path}: {e}. Skipping.\")\n",
    "    else:\n",
    "        print(f\"    âš ï¸ File not found for {hospital_id} at {feature_file_path}. Skipping.\")\n",
    "\n",
    "# Convert to NumPy arrays.\n",
    "if not feature_vectors:\n",
    "    print(\"\\nError: No feature files were found. Exiting script.\")\n",
    "    exit()\n",
    "\n",
    "feature_vectors = np.array(feature_vectors)\n",
    "labels = np.array(labels)\n",
    "filenames = np.array(filenames)\n",
    "print(f\"\\nSuccessfully combined all features.\")\n",
    "print(f\"Shape of the combined data matrix: {feature_vectors.shape}\")\n",
    "\n",
    "# --- 4. Data Standardization ---\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"\\n--- Step 2: Standardizing Data ---\")\n",
    "scaler = StandardScaler()\n",
    "standardized_data = scaler.fit_transform(feature_vectors)\n",
    "print(\"âœ… Full data has been standardized (zero mean, unit variance).\\n\")\n",
    "\n",
    "# --- 5. t-SNE Analysis on Full Data ---\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"--- Step 3: Applying t-SNE to Full Dataset ---\")\n",
    "\n",
    "# Adjust perplexity based on full dataset size (must be less than number of samples).\n",
    "n_samples = len(feature_vectors)\n",
    "perplexity = min(30, max(5, n_samples // 3))  # Dynamic perplexity\n",
    "print(f\"Using perplexity={perplexity} for full t-SNE (adjusted for {n_samples} samples).\")\n",
    "\n",
    "# Initialize t-SNE with 2 components for 2D visualization.\n",
    "tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42, init='pca', n_jobs=-1)\n",
    "tsne_result = tsne.fit_transform(standardized_data)\n",
    "\n",
    "print(\"âœ… t-SNE fitting and transformation complete.\")\n",
    "print(f\"Shape of data after t-SNE: {tsne_result.shape}\")\n",
    "print(f\"KL divergence (cost function): {tsne.kl_divergence_:.4f}\")\n",
    "\n",
    "# --- 6. Subsampling for Visualization ---\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"\\n--- Step 4: Stratified Subsampling for Visualization ---\")\n",
    "sub_indices = []\n",
    "num_classes = 5  # Assuming 5 classes (C_0 to C_4)\n",
    "subsample_size_per_class = max(1, SUBSAMPLE_SIZE // num_classes)\n",
    "unique_labels = sorted(set(labels))\n",
    "\n",
    "for label in unique_labels:\n",
    "    label_indices = np.where(labels == label)[0]\n",
    "    n_label = len(label_indices)\n",
    "    n_sub_label = min(n_label, max(subsample_size_per_class, int(n_label * SUBSAMPLE_FRACTION)))\n",
    "    if n_label > 0:\n",
    "        sub_label_idx = np.random.choice(label_indices, size=n_sub_label, replace=False)\n",
    "        sub_indices.extend(sub_label_idx)\n",
    "\n",
    "sub_indices = np.array(sub_indices)\n",
    "sub_tsne_result = tsne_result[sub_indices]\n",
    "sub_labels = labels[sub_indices]\n",
    "sub_filenames = filenames[sub_indices]\n",
    "print(f\"    âœ… Subsampled {len(sub_indices)} points for visualization.\")\n",
    "print(f\"    Subsampled t-SNE shape: {sub_tsne_result.shape}\")\n",
    "\n",
    "# --- 7. Calculate Centroids, Euclidean Distances, and Cosine Similarities ---\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"\\n--- Step 5: Calculating Centroids, Euclidean Distances, and Cosine Similarities ---\")\n",
    "\n",
    "# Calculate centroids for each class in t-SNE space (using subsampled data for visualization).\n",
    "centroids = {}\n",
    "for label in unique_labels:\n",
    "    indices = np.where(sub_labels == label)[0]\n",
    "    centroid = np.mean(sub_tsne_result[indices], axis=0)\n",
    "    centroids[label] = centroid\n",
    "    print(f\"Centroid for {label}: {centroid}\")\n",
    "\n",
    "# Compute pairwise Euclidean distances between centroids.\n",
    "centroid_coords = np.array([centroids[label] for label in unique_labels])\n",
    "euclidean_distances = squareform(pdist(centroid_coords, metric='euclidean'))\n",
    "euclidean_distance_df = pd.DataFrame(euclidean_distances, index=unique_labels, columns=unique_labels)\n",
    "print(\"\\nPairwise Euclidean distances between centroids:\")\n",
    "print(euclidean_distance_df)\n",
    "\n",
    "# Save the Euclidean distance matrix to a CSV file.\n",
    "euclidean_distance_output_path = os.path.join(OUTPUT_DIR, 'centroid_euclidean_distances.csv')\n",
    "euclidean_distance_df.to_csv(euclidean_distance_output_path)\n",
    "print(f\"âœ… Euclidean centroid distances saved to: {euclidean_distance_output_path}\")\n",
    "\n",
    "# Compute pairwise cosine similarities between centroids.\n",
    "cosine_similarities = np.zeros((len(unique_labels), len(unique_labels)))\n",
    "for i, label1 in enumerate(unique_labels):\n",
    "    for j, label2 in enumerate(unique_labels):\n",
    "        if i == j:\n",
    "            cosine_similarities[i, j] = 1.0\n",
    "        else:\n",
    "            cosine_similarities[i, j] = 1 - cosine(centroid_coords[i], centroid_coords[j])\n",
    "\n",
    "cosine_similarity_df = pd.DataFrame(cosine_similarities, index=unique_labels, columns=unique_labels)\n",
    "print(\"\\nPairwise Cosine similarities between centroids:\")\n",
    "print(cosine_similarity_df)\n",
    "\n",
    "# Save the cosine similarity matrix to a CSV file.\n",
    "cosine_similarity_output_path = os.path.join(OUTPUT_DIR, 'centroid_cosine_similarities.csv')\n",
    "cosine_similarity_df.to_csv(cosine_similarity_output_path)\n",
    "print(f\"âœ… Cosine similarities saved to: {cosine_similarity_output_path}\")\n",
    "\n",
    "# --- 8. Generate Excel File with Full t-SNE Results ---\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"\\n--- Step 6: Generating Excel File with Full t-SNE Results ---\")\n",
    "\n",
    "# Create a DataFrame for all samples' t-SNE coordinates and metadata.\n",
    "excel_data_full = {\n",
    "    'Filename': [],\n",
    "    'Path': [],\n",
    "    'tSNE_X': [],\n",
    "    'tSNE_Y': [],\n",
    "    'Dataset': [],\n",
    "    'Inner_Label': [],\n",
    "    'Dist_to_C_0': [],\n",
    "    'Dist_to_C_1': [],\n",
    "    'Dist_to_C_2': [],\n",
    "    'Dist_to_C_3': [],\n",
    "    'Dist_to_C_4': []\n",
    "}\n",
    "\n",
    "# Use vectorized distance calculation for all samples to centroids.\n",
    "all_distances = cdist(tsne_result, centroid_coords, metric='euclidean')\n",
    "\n",
    "for idx in range(len(filenames)):\n",
    "    # Extract filename and path.\n",
    "    file_path = filenames[idx]\n",
    "    file_name = os.path.basename(file_path)\n",
    "    \n",
    "    # Extract inner label (L_0 or L_1) from the path.\n",
    "    inner_label = 'L_0' if 'L_0' in file_path else 'L_1' if 'L_1' in file_path else 'Unknown'\n",
    "    \n",
    "    excel_data_full['Filename'].append(file_name)\n",
    "    excel_data_full['Path'].append(file_path)\n",
    "    excel_data_full['tSNE_X'].append(tsne_result[idx, 0])\n",
    "    excel_data_full['tSNE_Y'].append(tsne_result[idx, 1])\n",
    "    excel_data_full['Dataset'].append(labels[idx])\n",
    "    excel_data_full['Inner_Label'].append(inner_label)\n",
    "    \n",
    "    # Assign distances to each centroid.\n",
    "    for j, target_label in enumerate(unique_labels):\n",
    "        excel_data_full[f'Dist_to_{target_label}'].append(all_distances[idx, j])\n",
    "\n",
    "# Convert to DataFrame and save to Excel.\n",
    "excel_df_full = pd.DataFrame(excel_data_full)\n",
    "excel_full_output_path = os.path.join(OUTPUT_DIR, 'all_data_with_tsne_and_centroid_distances.xlsx')\n",
    "excel_df_full.to_excel(excel_full_output_path, index=False, engine='openpyxl')\n",
    "print(f\"âœ… Excel file with full t-SNE results saved to: {excel_full_output_path}\")\n",
    "\n",
    "# --- 9. Generate Excel File with C_2 Sample Distances (Separate t-SNE) ---\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"\\n--- Step 7: Generating Excel File with C_2 Sample Distances ---\")\n",
    "\n",
    "# Perform t-SNE on full C_2 data to get coordinates for distance calculations.\n",
    "c2_indices_full = np.where(labels == 'C_2')[0]\n",
    "if len(c2_indices_full) == 0:\n",
    "    print(\"\\nWarning: No C_2 samples found in the data. Skipping C_2 Excel output.\")\n",
    "else:\n",
    "    print(f\"--> Performing t-SNE on {len(c2_indices_full)} C_2 samples for distance calculations...\")\n",
    "    c2_features = feature_vectors[c2_indices_full]\n",
    "    c2_standardized = scaler.fit_transform(c2_features)\n",
    "    \n",
    "    # Adjust perplexity for C_2 samples.\n",
    "    c2_perplexity = min(30, max(5, len(c2_indices_full) // 3))\n",
    "    print(f\"Using perplexity={c2_perplexity} for C_2 t-SNE.\")\n",
    "    \n",
    "    c2_tsne = TSNE(n_components=2, perplexity=c2_perplexity, random_state=42, init='pca', n_jobs=-1)\n",
    "    c2_tsne_result = c2_tsne.fit_transform(c2_standardized)\n",
    "    print(f\"âœ… C_2 t-SNE complete. Shape: {c2_tsne_result.shape}\")\n",
    "\n",
    "    # Create a DataFrame for the C_2 Excel file with distances to all centroids.\n",
    "    excel_data_c2 = {\n",
    "        'Filename': [],\n",
    "        'Path': [],\n",
    "        'tSNE_X': [],\n",
    "        'tSNE_Y': [],\n",
    "        'Dataset': [],\n",
    "        'Inner_Label': [],\n",
    "        'Dist_to_C_0': [],\n",
    "        'Dist_to_C_1': [],\n",
    "        'Dist_to_C_2': [],\n",
    "        'Dist_to_C_3': [],\n",
    "        'Dist_to_C_4': []\n",
    "    }\n",
    "\n",
    "    # Use vectorized distance calculation.\n",
    "    c2_distances = cdist(c2_tsne_result, centroid_coords, metric='euclidean')\n",
    "    \n",
    "    for idx, orig_idx in enumerate(c2_indices_full):\n",
    "        file_path = filenames[orig_idx]\n",
    "        file_name = os.path.basename(file_path)\n",
    "        inner_label = 'L_0' if 'L_0' in file_path else 'L_1' if 'L_1' in file_path else 'Unknown'\n",
    "        \n",
    "        excel_data_c2['Filename'].append(file_name)\n",
    "        excel_data_c2['Path'].append(file_path)\n",
    "        excel_data_c2['tSNE_X'].append(c2_tsne_result[idx, 0])\n",
    "        excel_data_c2['tSNE_Y'].append(c2_tsne_result[idx, 1])\n",
    "        excel_data_c2['Dataset'].append('C_2')\n",
    "        excel_data_c2['Inner_Label'].append(inner_label)\n",
    "        \n",
    "        # Assign distances to each centroid.\n",
    "        for j, target_label in enumerate(unique_labels):\n",
    "            excel_data_c2[f'Dist_to_{target_label}'].append(c2_distances[idx, j])\n",
    "\n",
    "    # Convert to DataFrame and save to Excel.\n",
    "    excel_df_c2 = pd.DataFrame(excel_data_c2)\n",
    "    excel_c2_output_path = os.path.join(OUTPUT_DIR, 'C2_data_with_all_centroids_ViT_tSNE.xlsx')\n",
    "    excel_df_c2.to_excel(excel_c2_output_path, index=False, engine='openpyxl')\n",
    "    print(f\"âœ… Excel file with C_2 sample distances saved to: {excel_c2_output_path}\")\n",
    "\n",
    "# --- 10. Visualization ---\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"\\n--- Step 8: Visualizing t-SNE Results and Centroid Metrics ---\")\n",
    "\n",
    "# Scatter plot of t-SNE results with centroids (using subsampled points).\n",
    "try:\n",
    "    plt.style.use('ggplot')\n",
    "except OSError:\n",
    "    plt.style.use('default')\n",
    "    plt.rcParams['grid.linestyle'] = '--'\n",
    "    plt.rcParams['grid.linewidth'] = 0.5\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "# Plot subsampled t-SNE points.\n",
    "palette = sns.color_palette(\"deep\", n_colors=len(unique_labels))\n",
    "color_map = dict(zip(unique_labels, palette))\n",
    "for label, color in color_map.items():\n",
    "    sub_label_mask = (sub_labels == label)\n",
    "    ax.scatter(\n",
    "        sub_tsne_result[sub_label_mask, 0],\n",
    "        sub_tsne_result[sub_label_mask, 1],\n",
    "        label=label,\n",
    "        color=color,\n",
    "        alpha=0.6,\n",
    "        s=15\n",
    "    )\n",
    "\n",
    "# Plot centroids (from subsampled data).\n",
    "for label, centroid in centroids.items():\n",
    "    ax.scatter(\n",
    "        centroid[0], centroid[1],\n",
    "        color=color_map[label],\n",
    "        marker='*',\n",
    "        s=200,\n",
    "        edgecolors='black',\n",
    "        label=f'{label} Centroid'\n",
    "    )\n",
    "\n",
    "ax.set_title('2D t-SNE of ViT Feature Embeddings by Class with Centroids (Subsampled Points)', fontsize=18, fontweight='bold')\n",
    "ax.set_xlabel('tSNE_X', fontsize=12)\n",
    "ax.set_ylabel('tSNE_Y', fontsize=12)\n",
    "ax.legend(title='Class ID', fontsize=10)\n",
    "ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "tsne_plot_path = os.path.join(OUTPUT_DIR, 'tsne_visualization_with_centroids.png')\n",
    "plt.savefig(tsne_plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"âœ… t-SNE visualization saved to: {tsne_plot_path}\")\n",
    "\n",
    "# Heatmap of Euclidean distances.\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(euclidean_distance_df, annot=True, cmap='viridis', fmt='.2f', cbar_kws={'label': 'Euclidean Distance'})\n",
    "plt.title('Pairwise Euclidean Distances Between Class Centroids in t-SNE Space', fontsize=16)\n",
    "plt.xlabel('Class ID', fontsize=12)\n",
    "plt.ylabel('Class ID', fontsize=12)\n",
    "\n",
    "euclidean_heatmap_path = os.path.join(OUTPUT_DIR, 'centroid_euclidean_distances_heatmap.png')\n",
    "plt.savefig(euclidean_heatmap_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"âœ… Euclidean distances heatmap saved to: {euclidean_heatmap_path}\")\n",
    "\n",
    "# Heatmap of cosine similarities.\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cosine_similarity_df, annot=True, cmap='viridis', fmt='.2f', cbar_kws={'label': 'Cosine Similarity'})\n",
    "plt.title('Pairwise Cosine Similarities Between Class Centroids in t-SNE Space', fontsize=16)\n",
    "plt.xlabel('Class ID', fontsize=12)\n",
    "plt.ylabel('Class ID', fontsize=12)\n",
    "\n",
    "cosine_heatmap_path = os.path.join(OUTPUT_DIR, 'centroid_cosine_similarities_heatmap.png')\n",
    "plt.savefig(cosine_heatmap_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"âœ… Cosine similarities heatmap saved to: {cosine_heatmap_path}\")\n",
    "\n",
    "# Close all figures to free memory.\n",
    "plt.close('all')\n",
    "\n",
    "# --- 11. Save Processed Data for Future Use ---\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"\\n--- Step 9: Saving t-SNE Results and Labels ---\")\n",
    "\n",
    "tsne_output_path = os.path.join(OUTPUT_DIR, 'tsne_result_full.npy')\n",
    "np.save(tsne_output_path, tsne_result)\n",
    "print(f\"âœ… Full t-SNE transformed data saved to: {tsne_output_path}\")\n",
    "\n",
    "labels_output_path = os.path.join(OUTPUT_DIR, 'labels_full.npy')\n",
    "np.save(labels_output_path, labels)\n",
    "print(f\"âœ… Full labels saved to: {labels_output_path}\")\n",
    "\n",
    "print(\"\\n--- Script Finished Successfully ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a1a0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build TSNE dataset \n",
    "\n",
    "# ==============================================================================\n",
    "# SCRIPT: ViT_Feature_tSNE_Analysis_Full_C0_to_C5_NoCentroids.py\n",
    "# DESCRIPTION: This script performs t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "#              on the full set of Vision Transformer (ViT) feature vectors from all classes\n",
    "#              (C_0 to C_5, 378,354 + C_5 samples) without subsampling. It generates an Excel\n",
    "#              file with filename, full file path, t-SNE coordinates, dataset (C_0 to C_5),\n",
    "#              and inner label (L_0 or L_1) for all samples. A scatter plot visualizes\n",
    "#              subsampled points (1,000 total). Outputs are saved in a new folder. Compatible\n",
    "#              with .pkl files containing (full_path, feature, dataset_label, inner_label)\n",
    "#              tuples. Uses float16 to reduce memory usage and skips centroid distance\n",
    "#              calculations to avoid MemoryError.\n",
    "# AUTHOR:\n",
    "# DATE: 2025-10-01\n",
    "# ==============================================================================\n",
    "\n",
    "# --- 1. Import Necessary Libraries ---\n",
    "# ------------------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# --- 2. Configuration and Setup ---\n",
    "# ------------------------------------------------------------------------------\n",
    "# Define the base directory where feature files are located.\n",
    "FEATURE_DIR = r\"d:\\.kaggle\\new_FE_ViT_with_Path_and_Labels\"\n",
    "\n",
    "# Define the directory where all outputs (plots, data files) will be saved.\n",
    "OUTPUT_DIR = r\"d:\\.kaggle\\outputs_newRR5_vit_tsne_full\"\n",
    "\n",
    "# Create the output directory if it doesn't already exist.\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Subsampling configuration for visualization only\n",
    "SUBSAMPLE_SIZE = 1000  # Maximum total samples for visualization\n",
    "SUBSAMPLE_FRACTION = 0.1  # Fraction per class for visualization\n",
    "\n",
    "print(\"--- Configuration Complete ---\")\n",
    "print(f\"Feature source directory: {FEATURE_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\\n\")\n",
    "\n",
    "# --- 3. Data Loading and Preprocessing ---\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"--- Step 1: Loading and Preparing Data ---\")\n",
    "\n",
    "# Initialize lists to aggregate data from all files.\n",
    "labels = []\n",
    "feature_vectors = []\n",
    "filenames = []\n",
    "dataset_labels = []\n",
    "inner_labels = []\n",
    "\n",
    "# Loop through the 6 classes (C_0 to C_5).\n",
    "for i in range(6):\n",
    "    hospital_id = f\"C_{i}\"\n",
    "    \n",
    "    # Construct the full path to the feature file.\n",
    "    feature_file_path = os.path.join(FEATURE_DIR, f\"2features_vit_{hospital_id}.pkl\")\n",
    "\n",
    "    # Check if the file exists before trying to open it.\n",
    "    if os.path.exists(feature_file_path):\n",
    "        print(f\"--> Loading features for {hospital_id} from {feature_file_path}...\")\n",
    "        try:\n",
    "            with open(feature_file_path, \"rb\") as f:\n",
    "                # Load the data from the pickle file.\n",
    "                data = pickle.load(f)\n",
    "                \n",
    "                # Validate .pkl file format\n",
    "                if not data or not all(isinstance(item, tuple) and len(item) == 4 for item in data):\n",
    "                    print(f\"    âš ï¸ Invalid format in {feature_file_path}. Expected tuples of (full_path, feature, dataset_label, inner_label).\")\n",
    "                    continue\n",
    "                \n",
    "                # Extract the feature vectors, filenames, dataset labels, and inner labels.\n",
    "                class_features = [item[1] for item in data]\n",
    "                class_filenames = [item[0] for item in data]\n",
    "                class_datasets = [item[2] for item in data]\n",
    "                class_inner_labels = [item[3] for item in data]\n",
    "                \n",
    "                # Validate dataset labels\n",
    "                if not all(ds == hospital_id for ds in class_datasets):\n",
    "                    print(f\"    âš ï¸ Warning: Some dataset labels in {feature_file_path} do not match {hospital_id}.\")\n",
    "                \n",
    "                # Append to master lists.\n",
    "                feature_vectors.extend(class_features)\n",
    "                filenames.extend(class_filenames)\n",
    "                dataset_labels.extend(class_datasets)\n",
    "                inner_labels.extend(class_inner_labels)\n",
    "                labels.extend([hospital_id] * len(class_features))\n",
    "                \n",
    "                print(f\"    âœ… Loaded {len(class_features)} feature vectors for {hospital_id}.\")\n",
    "                # Report any 'Unknown' inner labels\n",
    "                unknown_count = sum(1 for il in class_inner_labels if il == 'Unknown')\n",
    "                if unknown_count > 0:\n",
    "                    print(f\"    âš ï¸ Found {unknown_count} 'Unknown' inner labels in {hospital_id}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"    âš ï¸ Error loading {feature_file_path}: {e}. Skipping.\")\n",
    "    else:\n",
    "        print(f\"    âš ï¸ File not found for {hospital_id} at {feature_file_path}. Skipping.\")\n",
    "\n",
    "# Convert to NumPy arrays with float16 to reduce memory usage\n",
    "if not feature_vectors:\n",
    "    print(\"\\nError: No feature files were found. Exiting script.\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    feature_vectors = np.array(feature_vectors, dtype=np.float16)\n",
    "    labels = np.array(labels)\n",
    "    filenames = np.array(filenames)\n",
    "    dataset_labels = np.array(dataset_labels)\n",
    "    inner_labels = np.array(inner_labels)\n",
    "except MemoryError as e:\n",
    "    print(f\"\\nError: MemoryError during array conversion: {e}\")\n",
    "    print(\"Consider running on a system with 32+ GB RAM or a cloud instance (e.g., AWS EC2 r5.2xlarge with 64 GB RAM).\")\n",
    "    exit()\n",
    "\n",
    "print(f\"\\nSuccessfully combined all features.\")\n",
    "print(f\"Shape of the combined data matrix: {feature_vectors.shape}\")\n",
    "\n",
    "# --- 4. Data Standardization ---\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"\\n--- Step 2: Standardizing Data ---\")\n",
    "scaler = StandardScaler()\n",
    "try:\n",
    "    standardized_data = scaler.fit_transform(feature_vectors)\n",
    "except MemoryError as e:\n",
    "    print(f\"\\nError: MemoryError during standardization: {e}\")\n",
    "    print(\"Consider running on a system with 32+ GB RAM or a cloud instance.\")\n",
    "    exit()\n",
    "print(\"âœ… Full data has been standardized (zero mean, unit variance).\\n\")\n",
    "\n",
    "# --- 5. t-SNE Analysis on Full Data ---\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"--- Step 3: Applying t-SNE to Full Dataset ---\")\n",
    "\n",
    "# Adjust perplexity based on dataset size (lower to reduce memory)\n",
    "n_samples = len(feature_vectors)\n",
    "perplexity = min(10, max(5, n_samples // 3))  # Lower perplexity to reduce memory\n",
    "print(f\"Using perplexity={perplexity} for full t-SNE (adjusted for {n_samples} samples).\")\n",
    "\n",
    "# Initialize t-SNE with 2 components\n",
    "try:\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42, init='pca', n_jobs=-1)\n",
    "    tsne_result = tsne.fit_transform(standardized_data)\n",
    "except MemoryError as e:\n",
    "    print(f\"\\nError: MemoryError during t-SNE computation: {e}\")\n",
    "    print(f\"t-SNE on {n_samples} samples requires significant memory. Consider:\")\n",
    "    print(\"- Running on a system with 32+ GB RAM or a cloud instance (e.g., AWS EC2 r5.2xlarge).\")\n",
    "    print(\"- Using UMAP instead (contact for alternative script).\")\n",
    "    exit()\n",
    "\n",
    "print(\"âœ… t-SNE fitting and transformation complete.\")\n",
    "print(f\"Shape of data after t-SNE: {tsne_result.shape}\")\n",
    "print(f\"KL divergence (cost function): {tsne.kl_divergence_:.4f}\")\n",
    "\n",
    "# --- 6. Subsampling for Visualization ---\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"\\n--- Step 4: Subsampling for Visualization ---\")\n",
    "sub_indices = []\n",
    "num_classes = 5  # Updated to 6 classes (C_0 to C_4)\n",
    "subsample_size_per_class = max(1, SUBSAMPLE_SIZE // num_classes)\n",
    "unique_labels = sorted(set(labels))\n",
    "\n",
    "for label in unique_labels:\n",
    "    label_indices = np.where(labels == label)[0]\n",
    "    n_label = len(label_indices)\n",
    "    n_sub_label = min(n_label, max(subsample_size_per_class, int(n_label * SUBSAMPLE_FRACTION)))\n",
    "    if n_label > 0:\n",
    "        sub_label_idx = np.random.choice(label_indices, size=n_sub_label, replace=False)\n",
    "        sub_indices.extend(sub_label_idx)\n",
    "\n",
    "sub_indices = np.array(sub_indices)\n",
    "sub_tsne_result = tsne_result[sub_indices]\n",
    "sub_labels = labels[sub_indices]\n",
    "sub_filenames = filenames[sub_indices]\n",
    "print(f\"    âœ… Subsampled {len(sub_indices)} points for visualization.\")\n",
    "print(f\"    Subsampled t-SNE shape: {sub_tsne_result.shape}\")\n",
    "\n",
    "# --- 7. Generate Excel File with Full t-SNE Results ---\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"\\n--- Step 5: Generating Excel File with Full t-SNE Results ---\")\n",
    "\n",
    "# Create a DataFrame for all samples' t-SNE coordinates and metadata\n",
    "excel_data_full = {\n",
    "    'Filename': [],\n",
    "    'Path': [],\n",
    "    'tSNE_X': [],\n",
    "    'tSNE_Y': [],\n",
    "    'Dataset': [],\n",
    "    'Inner_Label': []\n",
    "}\n",
    "\n",
    "for idx in range(len(filenames)):\n",
    "    excel_data_full['Filename'].append(os.path.basename(filenames[idx]))\n",
    "    excel_data_full['Path'].append(filenames[idx])\n",
    "    excel_data_full['tSNE_X'].append(tsne_result[idx, 0])\n",
    "    excel_data_full['tSNE_Y'].append(tsne_result[idx, 1])\n",
    "    excel_data_full['Dataset'].append(dataset_labels[idx])\n",
    "    excel_data_full['Inner_Label'].append(inner_labels[idx])\n",
    "\n",
    "# Convert to DataFrame and save to Excel\n",
    "try:\n",
    "    excel_df_full = pd.DataFrame(excel_data_full)\n",
    "    excel_full_output_path = os.path.join(OUTPUT_DIR, 'all_data_with_tsne_C0_to_C5.xlsx')\n",
    "    excel_df_full.to_excel(excel_full_output_path, index=False, engine='openpyxl')\n",
    "    print(f\"âœ… Excel file with full t-SNE results saved to: {excel_full_output_path}\")\n",
    "except MemoryError as e:\n",
    "    print(f\"\\nError: MemoryError during Excel generation: {e}\")\n",
    "    print(\"Consider running on a system with 32+ GB RAM or a cloud instance.\")\n",
    "    exit()\n",
    "\n",
    "# --- 8. Visualization ---\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"\\n--- Step 6: Visualizing t-SNE Results ---\")\n",
    "\n",
    "# Scatter plot of t-SNE results (subsampled points)\n",
    "try:\n",
    "    plt.style.use('ggplot')\n",
    "except OSError:\n",
    "    plt.style.use('default')\n",
    "    plt.rcParams['grid.linestyle'] = '--'\n",
    "    plt.rcParams['grid.linewidth'] = 0.5\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "# Plot subsampled t-SNE points\n",
    "palette = sns.color_palette(\"deep\", n_colors=len(unique_labels))\n",
    "color_map = dict(zip(unique_labels, palette))\n",
    "for label, color in color_map.items():\n",
    "    sub_label_mask = (sub_labels == label)\n",
    "    ax.scatter(\n",
    "        sub_tsne_result[sub_label_mask, 0],\n",
    "        sub_tsne_result[sub_label_mask, 1],\n",
    "        label=label,\n",
    "        color=color,\n",
    "        alpha=0.6,\n",
    "        s=15\n",
    "    )\n",
    "\n",
    "ax.set_title('2D t-SNE of ViT Feature Embeddings by Class (Subsampled, C_0 to C_5)', fontsize=18, fontweight='bold')\n",
    "ax.set_xlabel('tSNE_X', fontsize=12)\n",
    "ax.set_ylabel('tSNE_Y', fontsize=12)\n",
    "ax.legend(title='Class ID', fontsize=10)\n",
    "ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "tsne_plot_path = os.path.join(OUTPUT_DIR, 'tsne_visualization.png')\n",
    "plt.savefig(tsne_plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"âœ… t-SNE visualization saved to: {tsne_plot_path}\")\n",
    "\n",
    "# Close all figures to free memory\n",
    "plt.close('all')\n",
    "\n",
    "# --- 9. Save Processed Data for Future Use ---\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"\\n--- Step 7: Saving t-SNE Results and Labels ---\")\n",
    "\n",
    "tsne_output_path = os.path.join(OUTPUT_DIR, 'tsne_result_full.npy')\n",
    "np.save(tsne_output_path, tsne_result)\n",
    "print(f\"âœ… Full t-SNE transformed data saved to: {tsne_output_path}\")\n",
    "\n",
    "labels_output_path = os.path.join(OUTPUT_DIR, 'labels_full.npy')\n",
    "np.save(labels_output_path, labels)\n",
    "print(f\"âœ… Full labels saved to: {labels_output_path}\")\n",
    "\n",
    "print(\"\\n--- Script Finished Successfully ---\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
